{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c10468",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(__file__).parent.parent.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0036b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "MODEL_PATH = BASE_DIR / \"models\" / \"wgisd-grape-cluster-detection-v1.pt\"  # tu modelo YOLO entrenado\n",
    "IMAGE_DIR = BASE_DIR / \"data\" / \"yolo\" / \"images\" / \"val\"\n",
    "OUTPUT_CROP_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "os.makedirs(OUTPUT_CROP_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304eed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CARGAR MODELO ---\n",
    "model = YOLO(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e0ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIONES AUXILIARES ---\n",
    "def save_crop(img, box, output_path, idx):\n",
    "    \"\"\"Recorta el BB de la imagen y lo guarda.\"\"\"\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    crop = img[y1:y2, x1:x2]\n",
    "    cv2.imwrite(os.path.join(output_path, f\"crop_{idx:03d}.jpg\"), crop)\n",
    "    return crop, (x1, y1, x2, y2)\n",
    "\n",
    "def apply_mask_to_original(mask, box, original_shape):\n",
    "    \"\"\"\n",
    "    Pega una máscara (binary mask) sobre la imagen original\n",
    "    en la posición del bounding box.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    orig_mask = np.zeros((original_shape[0], original_shape[1]), dtype=np.uint8)\n",
    "    # Redimensionar máscara al tamaño del BB original\n",
    "    mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "    orig_mask[y1:y2, x1:x2] = mask_resized\n",
    "    return orig_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92af391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 24 uva_bboxs, 29.7ms\n",
      "Speed: 2.8ms preprocess, 29.7ms inference, 4.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 18 uva_bboxs, 4.4ms\n",
      "Speed: 1.5ms preprocess, 4.4ms inference, 3.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 10 uva_bboxs, 4.3ms\n",
      "Speed: 1.5ms preprocess, 4.3ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 10 uva_bboxs, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 24 uva_bboxs, 4.3ms\n",
      "Speed: 1.7ms preprocess, 4.3ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 13 uva_bboxs, 4.6ms\n",
      "Speed: 1.8ms preprocess, 4.6ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 14 uva_bboxs, 4.5ms\n",
      "Speed: 2.8ms preprocess, 4.5ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 19 uva_bboxs, 4.3ms\n",
      "Speed: 2.0ms preprocess, 4.3ms inference, 4.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 31 uva_bboxs, 35.5ms\n",
      "Speed: 1.5ms preprocess, 35.5ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 11 uva_bboxs, 5.0ms\n",
      "Speed: 3.3ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 6 uva_bboxs, 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 16 uva_bboxs, 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 17 uva_bboxs, 4.3ms\n",
      "Speed: 1.6ms preprocess, 4.3ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 11 uva_bboxs, 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 20 uva_bboxs, 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 9 uva_bboxs, 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 26 uva_bboxs, 4.3ms\n",
      "Speed: 1.5ms preprocess, 4.3ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 28 uva_bboxs, 4.3ms\n",
      "Speed: 2.6ms preprocess, 4.3ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 8 uva_bboxs, 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 17 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 17 uva_bboxs, 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 2.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 8 uva_bboxs, 4.5ms\n",
      "Speed: 2.0ms preprocess, 4.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 21 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 3.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 23 uva_bboxs, 4.3ms\n",
      "Speed: 1.5ms preprocess, 4.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 15 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 16 uva_bboxs, 5.6ms\n",
      "Speed: 2.7ms preprocess, 5.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 13 uva_bboxs, 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 2.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 uva_bboxs, 4.3ms\n",
      "Speed: 1.8ms preprocess, 4.3ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 19 uva_bboxs, 4.3ms\n",
      "Speed: 1.6ms preprocess, 4.3ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 17 uva_bboxs, 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 15 uva_bboxs, 4.6ms\n",
      "Speed: 1.5ms preprocess, 4.6ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 21 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 25 uva_bboxs, 4.3ms\n",
      "Speed: 2.3ms preprocess, 4.3ms inference, 4.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 27 uva_bboxs, 4.3ms\n",
      "Speed: 1.7ms preprocess, 4.3ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 12 uva_bboxs, 4.5ms\n",
      "Speed: 1.8ms preprocess, 4.5ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 20 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 3.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 28 uva_bboxs, 4.6ms\n",
      "Speed: 1.7ms preprocess, 4.6ms inference, 4.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 14 uva_bboxs, 4.3ms\n",
      "Speed: 1.7ms preprocess, 4.3ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 18 uva_bboxs, 4.3ms\n",
      "Speed: 3.5ms preprocess, 4.3ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 15 uva_bboxs, 4.3ms\n",
      "Speed: 1.6ms preprocess, 4.3ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 16 uva_bboxs, 4.5ms\n",
      "Speed: 1.9ms preprocess, 4.5ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 11 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 17 uva_bboxs, 5.2ms\n",
      "Speed: 2.7ms preprocess, 5.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 21 uva_bboxs, 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 4.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 12 uva_bboxs, 4.3ms\n",
      "Speed: 1.7ms preprocess, 4.3ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 16 uva_bboxs, 4.3ms\n",
      "Speed: 1.7ms preprocess, 4.3ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 uva_bboxs, 4.3ms\n",
      "Speed: 1.8ms preprocess, 4.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 13 uva_bboxs, 4.6ms\n",
      "Speed: 1.8ms preprocess, 4.6ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 uva_bboxs, 4.5ms\n",
      "Speed: 2.1ms preprocess, 4.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 22 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 13 uva_bboxs, 4.3ms\n",
      "Speed: 2.5ms preprocess, 4.3ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 11 uva_bboxs, 5.2ms\n",
      "Speed: 2.4ms preprocess, 5.2ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 14 uva_bboxs, 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 22 uva_bboxs, 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 12 uva_bboxs, 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 23 uva_bboxs, 4.2ms\n",
      "Speed: 1.4ms preprocess, 4.2ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 12 uva_bboxs, 4.3ms\n",
      "Speed: 1.5ms preprocess, 4.3ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 24 uva_bboxs, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Recortes y máscaras iniciales generadas.\n"
     ]
    }
   ],
   "source": [
    "# --- PIPELINE ---\n",
    "for img_name in os.listdir(IMAGE_DIR):\n",
    "    if not img_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    results = model.predict(img, imgsz=640)  # retorna lista de Results\n",
    "    results = results[0]  # si hay un solo frame\n",
    "\n",
    "    boxes = results.boxes.xyxy.cpu().numpy()  # bounding boxes\n",
    "    OUTPUT_IMG_DIR = os.path.join(OUTPUT_CROP_DIR, os.path.splitext(img_name)[0])\n",
    "    os.makedirs(OUTPUT_IMG_DIR, exist_ok=True)\n",
    "\n",
    "    for idx, box in enumerate(boxes):\n",
    "        crop, box_coords = save_crop(img, box, OUTPUT_IMG_DIR, idx)\n",
    "\n",
    "        # --- AQUÍ se aplicaría la segmentación de colores ---\n",
    "        # Ejemplo: máscara dummy toda blanca (solo demostración)\n",
    "        dummy_mask = np.ones(crop.shape[:2], dtype=np.uint8) * 255\n",
    "        mask_on_orig = apply_mask_to_original(dummy_mask, box_coords, img.shape)\n",
    "\n",
    "        # Opcional: guardar máscara sobre la imagen original\n",
    "        mask_vis_path = os.path.join(OUTPUT_IMG_DIR, f\"mask_on_orig_{idx:03d}.png\")\n",
    "        cv2.imwrite(mask_vis_path, mask_on_orig)\n",
    "\n",
    "print(\"Recortes y máscaras iniciales generadas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-aplicada-venv311 (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
